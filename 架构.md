
# id全局唯一且自增，如何实现？
Redis的 incr 和 increby 自增原子命令
统一数据库的id发放
美团Leaf
Leaf——美团点评分布式ID生成系统
Twitter的snowflake算法



# 如何设计算法压缩一段URL？
通过发号策略，给每一个过来的长地址，发一个号即可，小型系统直接用mysql的自增索引就搞定了。  
如果是大型应用，可以考虑各种分布式key-value系统做发号器。不停的自增就行了。第一个使用这个  
服务的人得到的短地址是http://xx.xx/0 第二个是 http://xx.xx/1 第11个是 http://xx.xx/a   
第依次往后，相当于实现了一个62进制的自增字段即可。

常用的url压缩算法是短地址映射法。具体步骤是：  

1. 将长网址用md5算法生成32位签名串，分为4段,，每段8个字符；
2. 对这4段循环处理，取每段的8个字符, 将他看成16进制字符串与0x3fffffff(30位1)的位与操作，超过30位的忽略处理；
3. 将每段得到的这30位又分成6段，每5位的数字作为字母表的索引取得特定字符，依次进行获得6位字符串；
4. 这样一个md5字符串可以获得4个6位串，取里面的任意一个就可作为这个长url的短url地址。

# Dubbo负载均衡策略？
随机、轮询、最少使用、一致性哈希（除了一致性哈希外，都有加权）

## 负载均衡算法？

常见6种负载均衡算法：轮询，随机，源地址哈希，加权轮询，加权随机，最小连接数。

nginx5种负载均衡算法：轮询，weight，ip_hash，fair（响应时间），url_hash

dubbo负载均衡算法：随机，轮询，最少活跃调用数，一致性Hash

## Dubbo源码使用了哪些设计模式

A，工厂模式，ExtenstionLoader.getExtenstionLoader(Protocol.class).getAdaptiveExtenstion()

B，装饰器模式+责任链，以provider的调用链为例，具体调用链代码是在protocolFilterWrapper的buildInvokeChain完成的,将注解中含有group=provider的Filter实现，调用顺序为EchoFilter -> ClassLoaderFilter -> GenericFilter -> ContextFilter -> ExceptionFilter -> TimeoutFilter -> MonitorFilter -> TraceFilter。装饰器模式和责任链混合使用，Echo是回声测试请求，ClassLoaderFilter则只是在其主功能上添加了功能。

C，观察者模式，provider启动时需要与注册中心交互，先注册自己的服务，再订阅自己的服务，订阅时采用了观察者模式，注册中心每5s定时检查是否有服务更新，有更新则向服务提供者发送1个notify消息后即可运行NotifyListener的notity方法，执行监听器方法。

D，动态代理模式。  扩展JDK的ExtensionLoaderdeAdaptive实现，根据调用阶段动态参数决定调用哪个类，生成代理类的代码是ExtensionLoader的createAdaptiveExtenstionClassLoader方法。

# Dubbo中Zookeeper做注册中心，如果注册中心集群都挂掉，发布者和订阅者之间还能通信么？
可以，因为dubbo在注册中心挂掉之后，会从原先的缓存中读取连接地址。


# Dubbo完整的一次调用链路介绍？

    调用方：
    1. 将方法名方法参数传入InvokerInvocationHandler的invoke方法中，对于Object中的方法toString, hashCode, equals直接调用invoker的对应方法。
    2. 然后进入（故障转移集群）MockClusterInvoker.invoke()方法中。三种调用策略：①不需要mock， 直接调用FailoverClusterInvoker。②强制mock，调用mock。③先调FailoverClusterInvoker，调用失败在mock.
    3. FailoverClusterInvoker默认调用策略。①通过目录服务查找到所有订阅的服务提供者的Invoker对象。②路由服务根据策略（比如：容错策略）来过滤选择调用的Invokers。③通过负载均衡策略LoadBalance来选择一个Invoker
    4. 执行选择的Invoker.invoker(invocation),经过监听器链，经过过滤器链,执行到远程调用的DubboInvoker。
    5. DubboInvoker根据url 也就是根据服务提供者的长连接，这里封装成交互层对象ExchangeClient供这里调用，判断远程调用类型同步，异步还是oneway模式。ExchangeClient发起远程调用。
    6.获取调用结果：①Oneway返回空RpcResult②异步，直接返回空RpcResult, ResponseFuture回调③同步， ResponseFuture模式同步转异步，等待响应返回

    消费方：
    1.通过Invocation获取服务名和端口组成serviceKey=com.alibaba.dubbo.demo.DemoService:20880, 从DubboProtocol的exproterMap中获取暴露服务的DubboExporter, 在从dubboExporter 获取invoker返回
    2.经过过滤器链。
    3.经过监听器链。
    4.到达执行真正调用的invoker， 这个invoker由代理工厂ProxyFactory.getInvoker(demoService, DemoService.class, registryUrl)创建，具体请看代理那部分介绍。
    5.调用demoService实例方法，将结果封装成RpcResult返回。
    
    
# SpringCloud和Dubbo有什么不一样？
1.dubbo采用RPC的方式交互，SpringCloud采用Http,restful协议进行交互。
2.dubbo依赖zookeeper进行服务注册，Springloud自己拥有自己的服务注册中心。
3.dubbo需要强依赖，需要持有相同的类或者jar包，springcloud弱依赖，但需要通过接口文档进行约束。
4.C数据一致性，A服务可用性，P服务对网络分区故障的容错性，Zookeeper 保证的是CP，
euraka保证的是AP。


# 使用Redis如何实现分布式锁？

redis创建一个订单id，和客户端id，和生存时间。默认一个线程会一直延长这个生存时间。如果其他线程查询到这个值，
并且客户端的id不是自己的话，判断这个锁被别人用了。


# Tomcat如何优化？
虚拟机参数：1.server模式。2.最大堆最小堆大小。3.年轻代和老年代的比例。
4.开启优化。5.使用偏向锁。6.gc年龄。7.合适的gc
tomcat参数：1.maxThread。2.minThread。3.acceptCoun。4.connectionTimeout。6.maxProcessors与minProcessors。


# 后台系统怎么防止请求重复提交？
1.通过数据库订单的状态。（事务的情况）
2.缓存锁定订单号。
3.会话记录请求url
4.申请凭证。


# 后台系统怎么防止请求频繁？
根据用户id，存入一个用户id，和调用次数的值，值的过期时间为2秒。
当超过10的话，提示请求频繁

# 请谈谈单点登录原理？

同域下的单点登录，只需共享session即可。
登录业务系统，跳转至SSO服务器，判断用户名密码正确，在sso域下种下cookie，在session中标记为登录，返回一个ticket，跳转到业务系统，业务系统再拿这个ticket跑去SSO服务器验证ticket是否有效，有效的话，在业务系统session中设置为已登录即可。



相比于单系统登录，sso需要一个独立的认证中心，只有认证中心能接受用户的用户名密码等安全信息，其他系统不提供登录入口，只接受认证中心的间接授权。间接授权通过令牌实现，sso认证中心验证用户的用户名密码没问题，创建授权令牌，在接下来的跳转过程中，授权令牌作为参数发送给各个子系统，子系统拿到令牌，即得到了授权，可以借此创建局部会话，局部会话登录方式与单系统的登录方式相同。这个过程，也就是单点登录的原理，用下图说明



单点登录自然也要单点注销，在一个子系统中注销，所有子系统的会话都将被销毁，用下面的图来说明


# Linux常见命令有哪些？
cd、ls、grep、find、cp、mv、rm、ps、kill、killall、file、tar
cat、chgrp、chown、chmod、vim、gcc、time

# 请说说什么是Maven的依赖、继承以及聚合？
依赖是jar之间的依赖。
聚合是项目中多模块构建。
继承是子模块复用父模块的公共依赖。

# Git暂存区和工作区的区别？

git add命令实际上就是把要提交的所有修改放到暂存区（Stage）
git commit就可以一次性把暂存区的所有修改提交到分支


# Git如何创建、回退以及撤销版本？

git checkout -- filename
git reset --hard HEAD^

# 谈谈项目中分布式事务应用场景？
先理解CAP原理。
①分库分表的情况下
②一个事务涉及到订单的数据库和账户的数据库


## MQ和数据库的一致性问题。
MQ做数据同步也会造成不一致，又需要引入监控，实时计算2个集群的数据同步，做一致性同步。大部分来说，同步es和solr不要在代码中去同步，同步失败无法保证事务，而且业务耦合。可以使用Databug和cancel等工具去做代码解耦，MQ支持重试，存储失败后抛出异常下次再处理。数据做异构，对外服务时任意拼装，MYSQL在半同步复制上做了一些优化，保证了一致性，引入了诸如paxos等主流算法保证强一致性问题。
当DB（监听从库），binlog有变化，cancel监听到时候解析过滤发送MQ（表名字，主键等）到变化的实时从库中查询数据同步到ES聚合表，MQ可以重试，系统解耦。事务log挖掘县城会对DB的事务log监听，并把这些事件发布到消息代理。


## 正在处理的队列突然断电怎么办？

正在处理的实现事务功能，下次自动回滚。
队列实现持久化储存，下次启动自动载入。
添加标志位，未处理 0，处理中 1，已处理 2。每次启动的时候，把所有状态为 1 的，置为 0。
关键性的应用就给电脑配个 UPS。


## 服务限流的方式

    漏桶：水(请求)先进入到漏桶里,漏桶以一定的速度出水(接口有响应速率),当水流入速度过大会直接溢出(访问频率超过接口响应速率),然后就拒绝请求。
    令牌桶算法：系统会按恒定1/QPS时间间隔(如果QPS=100,则间隔是10ms)往桶里加入Token，如果桶已经满了就不再加了.新请求来临时,会各自拿走一个Token,如果没有Token就拒绝服务。
    基于redis实现的限流：假设每分钟访问次数不能超过10次，在Redis中创建一个键，过期60秒，对此服务接口的访问就把键值加1，在60秒内增加到10的时候，禁止访问服务接口。
    计数器，滑动窗口


## 如何设计幂等？

每次扣减库存时加上1个请求流水编号，上层请求扣减库存没拿到结果的话，重新查询1次做重试操作，量不大都是加锁处理。减少锁的时间，牺牲幂等性，扣减为DB下地操作，查询扣减和设置合成1步，中间没有网络请求。利用缓存，通过写log记录操作，异步合并日志及更新，重启时cache失效，读log恢复，避免重复提交，写操作不建议重试快速失败。多个商品同时增减库存，可使用订单号做幂等处理，应用层对单个商品减库存，操作排队，商品消息ID路由在1个应用server处理，读本地缓存，失效再redis，DB采用乐观锁，组提交，1次减库存多个订单的购买量。可将同一个key下库存m分为n组k1......kn，每组数为m/n，扣减依次在各组扣减，减少并发冲突。队列装满后关闭队列进入，然后用户轮训自己是否抢到了异步ajax，用户资源队列固定长度。2个队列，1个销售的资源队列放入redis，有另外1个队列用来装抢购的会员的uid。

红包状态正常，并成功将状态改为“已领取”，且消息发送成功，用户端开始消费该消息，如果消费失败/超时，用MQ做重试做幂等，直到成功，每条消息有唯一编号且保证消息处理成功与去重表的日志同时出现。

热点将hot data拆分，分在不同库和不同表，分散热点Data，减轻DB并发更新热点带来RT升高和应用连接超时。SQL在mysql层加以限制，SQL超时/thradrunning到1定值则拒绝SQL执行，一定时间异步将结果写入DB，nginx对IP做限制，可能误杀。

## RabbitMQ消息堆积怎么处理？

    增加消费者的处理能力(例如优化代码)，或减少发布频率
    单纯升级硬件不是办法，只能起到一时的作用
    考虑使用队列最大长度限制，RabbitMQ 3.1支持
    给消息设置年龄，超时就丢弃
    默认情况下，rabbitmq消费者为单线程串行消费，设置并发消费两个关键属性concurrentConsumers和prefetchCount，concurrentConsumers设置的是对每个listener在初始化的时候设置的并发消费者的个数，prefetchCount是每次一次性从broker里面取的待消费的消息的个数
    建立新的queue，消费者同时订阅新旧queue
    生产者端缓存数据，在mq被消费完后再发送到mq
    打破发送循环条件，设置合适的qos值，当qos值被用光，而新的ack没有被mq接收时，就可以跳出发送循环，去接收新的消息；消费者主动block接收进程，消费者感受到接收消息过快时主动block，利用block和unblock方法调节接收速率，当接收线程被block时，跳出发送循环。
    新建一个topic，partition是原来的10倍；然后写一个临时的分发数据的consumer程序，这个程序部署上去消费积压的数据，消费之后不做耗时的处理，直接均匀轮询写入临时建立好的10倍数量的queue；接着临时征用10倍的机器来部署consumer，每一批consumer消费一个临时queue的数据；等快速消费完积压数据之后，得恢复原先部署架构，重新用原先的consumer机器来消费消息；
    
    
## kafka消息会不会丢失？

Kafka消息发送分同步(sync)、异步(async)两种方式。默认是使用同步方式，可通过producer.type属性进行配置；Kafka保证消息被安全生产，有三个选项分别是0,1,-1。
通过request.required.acks属性进行配置：
0代表：不进行消息接收是否成功的确认(默认值)；
1代表：当Leader副本接收成功后，返回接收成功确认信息；
-1代表：当Leader和Follower副本都接收成功后，返回接收成功确认信息；

    网络异常
    acks设置为0时，不和Kafka集群进行消息接受确认，当网络发生异常等情况时，存在消息丢失的可能；
    客户端异常
    异步发送时，消息并没有直接发送至Kafka集群，而是在Client端按一定规则缓存并批量发送。在这期间，如果客户端发生死机等情况，都会导致消息的丢失；
    缓冲区满了
    异步发送时，Client端缓存的消息超出了缓冲池的大小，也存在消息丢失的可能；
    Leader副本异常
    acks设置为1时，Leader副本接收成功，Kafka集群就返回成功确认信息，而Follower副本可能还在同步。这时Leader副本突然出现异常，新Leader副本(原Follower副本)未能和其保持一致，就会出现消息丢失的情况；

以上就是消息丢失的几种情况，在日常应用中，我们需要结合自身的应用场景来选择不同的配置。
想要更高的吞吐量就设置：异步、ack=0；想要不丢失消息数据就选：同步、ack=-1策略


## RabbitMQ的消息丢失解决方案？

    消息持久化：Exchange 设置持久化：durable:true；Queue 设置持久化；Message持久化发送。
    ACK确认机制：消息发送确认；消息接收确认。
    
## kafka的leader副本选举？

如果某个分区patition的Leader挂了,那么其它跟随者将会进行选举产生一个新的leader,之后所有的读写就会转移到这个新的Leader上,在kafka中,其不是采用常见的多数选举的方式进行副本的Leader选举,而是会在Zookeeper上针对每个Topic维护一个称为ISR（in-sync replica，已同步的副本）的集合,显然还有一些副本没有来得及同步。只有这个ISR列表里面的才有资格成为leader(先使用ISR里面的第一个，如果不行依次类推，因为ISR里面的是同步副本，消息是最完整且各个节点都是一样的)。
  通过ISR,kafka需要的冗余度较低，可以容忍的失败数比较高。假设某个topic有f+1个副本，kafka可以容忍f个不可用,当然,如果全部ISR里面的副本都不可用,也可以选择其他可用的副本,只是存在数据的不一致。
  
  
## kafka消息的检索？

其实很简单主要是用二分查找算法,比如我们要查找一条offest=10000的文件,kafka首先会在对应分区下的log文件里采用二分查看定位到某个记录该offest
=10000这条消息的log,然后从相应的index文件定位其偏移量,然后拿着偏移量到log里面直接获取。这样就完成了一个消息的检索过程。


## RabbitMQ 集群方式？

1）普通集群：

以两个节点（rabbit01、rabbit02）为例来进行说明。
    rabbit01和rabbit02两个节点仅有相同的元数据，即队列的结构，但消息实体只存在于其中一个节点rabbit01（或者rabbit02）中。
    当消息进入rabbit01节点的Queue后，consumer从rabbit02节点消费时，RabbitMQ会临时在rabbit01、rabbit02间进行消息传输，把A中的消息实体取出并经过B发送给consumer。所以consumer应尽量连接每一个节点，从中取消息。即对于同一个逻辑队列，要在多个节点建立物理Queue。否则无论consumer连rabbit01或rabbit02，出口总在rabbit01，会产生瓶颈。当rabbit01节点故障后，rabbit02节点无法取到rabbit01节点中还未消费的消息实体。如果做了消息持久化，那么得等rabbit01节点恢复，然后才可被消费；如果没有持久化的话，就会产生消息丢失的现象。

2）镜像集群：

 在普通集群的基础上，把需要的队列做成镜像队列，消息实体会主动在镜像节点间同步，而不是在客户端取数据时临时拉取，也就是说多少节点消息就会备份多少份。该模式带来的副作用也很明显，除了降低系统性能外，如果镜像队列数量过多，加之大量的消息进入，集群内部的网络带宽将会被这种同步通讯大大消耗掉。所以在对可靠性要求较高的场合中适用
    由于镜像队列之间消息自动同步，且内部有选举master机制，即使master节点宕机也不会影响整个集群的使用，达到去中心化的目的，从而有效的防止消息丢失及服务不可用等问题
    
## kafka高性能的原因？

A，Broker NIO异步消息处理，实现了IO线程与业务线程分离；

B，磁盘顺序写；

C， 零拷贝（跳过用户缓冲区的拷贝，建立一个磁盘空间和内存的直接映射，数据不再复制到用户态缓冲区）；

D，分区/分段（每次文件操作都是对一个小文件的操作，非常轻便，同时也增加了并行处理能力）；

F，批量发送 (可以指定缓存的消息达到某个量的时候就发出去，或者缓存了固定的时间后就发送出去，大大减少服务端的I/O次数)

E，数据压缩


## ZooKeeper分布式高可用

ZooKeeper 运行期间，集群中至少有过半的机器保存了最新数据。集群超过半数的机器能够正常工作，集群就能够对外提供服务。

zookeeper可以选出N台机器作主机，它可以实现M:N的备份；keepalive只能选出1台机器作主机，所以keepalive只能实现M:1的备份。

通常有以下两种部署方案：双机房部署（一个稳定性更好、设备更可靠的机房，这个机房就是主要机房，而另外一个机房则更加廉价一些，例如，对于一个由 7 台机器组成的 ZooKeeper 集群，通常在主要机房中部署 4 台机器，剩下的 3 台机器部署到另外一个机房中）；三机房部署（无论哪个机房发生了故障，剩下两个机房的机器数量都超过半数。在三个机房中都部署若干个机器来组成一个 ZooKeeper 集群。假设机器总数为 N，各机房机器数：N1 = (N-1)/2 ，N2=1~(N-N1)/2 ，N3 = N - N1 - N2 ）。

水平扩容就是向集群中添加更多机器，Zookeeper2种方式（不完美），一种是集群整体重启，另外一种是逐台进行服务器的重启。


## redis和memcache的区别 
redis使用单线程模型，数据顺序提交，redis支持主从模式，mencache只支持一致性hash做分布式；redis支持数据落地，rdb定时快照和aof实时记录操作命令的日志备份，memcache不支持；redis数据类型丰富，有string，hash，set，list， sort set，而memcache只支持简单数据类型；memcache使用cas乐观锁做一致性。

jedis操作Hash：hmset, hmget, hdel, hkeys

jedis操作List： lpush，lrange按照范围取出，rpush， del， sort等keyjedis操作Set：sadd，srem移除noname，smembers， sismember， scard等。summary.note

使用场景例如

Hash：存储读取更新用户多个属性

List：微博TimeLine，消息列表

Set：共同好友，二度好友，用唯一性可以统计网站所有独立IP，好友推荐根据tag求交集，大于threshold就可以推荐。

sortset：set增加1个权重score参数

其他场景：A订阅发布系统，redis对某个key消息发布及订阅，当1个key消息发布后，所有订阅它的客户端都会收到相应消息，例如实时消息系统，即时聊天，群聊等。

事务-常用EX,EC提交执行的命令，在server不出问题，可以保证一连串的命令是顺序执行额；提供1个watch功能，对1个key作watch，然后再执行transation。



## redis过期淘汰策略

redis内存数据上升到一定大小会执行数据淘汰策略，redis提供了6种数据淘汰策略。

LRU：从已设置过期时间的数据集合中挑选最近最少使用的数据淘汰

random：从已设置过期时间的数据中挑选任意数据淘汰

ttl：从已设置过期时间的数据集合中挑选将要过期的数据淘汰。

notenvision：禁止驱逐数据

如mysql中有2千万数据，redis只存储20万的热门数据。LRU或者TTL都满足热点数据读取较多，不太可能超时特点。

redis特点：速度块，O(1)，丰富的数据类型，支持事物原子性，可用于缓存，比memecache速度块，可以持久化数据。

常见问题和解决：Master最好不做持久化如RDB快照和AOF日志文件；如果数据比较重要，某分slave开启AOF备份数据，策略为每秒1次，为了主从复制速度及稳定，MS主从在同一局域网内；主从复制不要用图状结构，用单向链表更为稳定 M-S-S-S-S。。。。；redis过期采用懒汉+定期，懒汉即get/set时候检查key是否过期，过期则删除key，定期遍历每个DB，检查制定个数个key；结合服务器性能调节并发情况。

过期淘汰，数据写入redis会附带1个有效时间，这个有效时间内该数据被认为是正确的并不关心真实情况，例如对支付等业务采用版本号实现，redis中每一份数据都维持1个版本号，DB中也维持1份，只有当redis的与DB中的版本一致时，才会认为redis为有效的，不过仍然每次都要访问DB，只需要查询version版本字段即可。


## 如何将数据分布在redis第几个库？

答：redis 本身支持16个数据库，通过 数据库id 设置，默认为0。
例如jedis客户端设置。一：JedisPool(org.apache.commons.pool.impl.GenericObjectPool.Config poolConfig, String host, int port, int timeout, String password, int database);
第一种通过指定构造函数database字段选择库，不设置则默认0库。二：jedis.select(index);调用jedis的select方法指定。



## 幂等的处理方式？

答：一、查询与删除操作是天然幂等

二、唯一索引，防止新增脏数据

三、token机制，防止页面重复提交

四、悲观锁  for update

五、乐观锁（通过版本号/时间戳实现， 通过条件限制where avai_amount-#subAmount# >= 0）

六、分布式锁

七、状态机幂等（如果状态机已经处于下一个状态，这时候来了一个上一个状态的变更，理论上是不能够变更的，这样的话，保证了有限状态机的幂等。）

八、select + insert（并发不高的后台系统，或者一些任务JOB，为了支持幂等，支持重复执行）


## 如何设计秒杀

异步化，生产接口每秒钟10万并发，消费者用异步慢慢消费。缓存模式空间换时间，把1两亿的数据名单打到缓存。服务降级，把不重要的任务放弃；静态资源离线包下载机制，在wify下会主动提前把静态下载前端层保护可请将用户请求延长，点击后主动给它随机等待2s的时间/2分钟之内不能请求；后端做部分接口的开关，设置超短耗时时间，原来只用5ms的只给20ms。

系统一段时间内会自动重试，重试多次后就认为是失败了，检查支付接口返回该订单的钱，支付操作如果回复错误则回滚扣库存的事务，没返回则会记录进行中pendding状态，结束整个过程，等通知失败/成功，AB系统之间会出现死循环补偿，如B退单不成功，一般就是记录错误日志了。超时每隔一段时间去定时回调服务定时回滚，一定次数还是超时则提示用户联系客服，订单库存可以不会滚，记录状态，如果一直调用支付不成功，则让用户自己去处理联系客服，可以不回滚用户的数据，金额扣了才算真正完成，是一种简单粗暴的做法。

公共配置抽象成存储到zookeeper配置中心或者redis等，DB也存储一份，各应用监听ZK的配置变化，可以建一个配置web管理页面。


## 高性能统计UV的方式？

（1）使用redis的set集合

（2）使用redis的bitmap（注意内存消耗）


## 缓存击透

预加载；

加载DB时同步，其他则等待；

DB端做SQL合并，Queue合并排队处理；

部分缓存设置为永不过期；

先清除缓存，读取数据时候则等待500ms，500ms缓存应该已经加载完成；

采用双key缓存，A1为原始缓存，A2为拷贝缓存；

如果DB为空null则g给redis设置1个NFC空nei容。


## Elasticsearch分片使用优化?

答：(1)拆分集群
对于存在明显分界线的业务，可以按照业务、地域使用不同集群，这种拆分集群的思路是非常靠谱的。对于我们的场景，已经按照地域拆分了集群，且同一地域的子业务间分界线不明显，拆分过多的集群维护成本较高。
(2)调整滚动周期
根据保留时长调整index滚动周期是最简单有效的思路。例如保留3天的数据按天滚动，保留31天的数据按周滚动，保留一年的数据按月滚动。合理的滚动周期，可以在存储成本增加不大的情况下，大幅降低分片数量。
对于我们的场景，大部分数据保留31天，在按周滚动的情况下，集群的总分片数可以下降到6.5w~个。
(3)合理设置分片数和副本数
除个别子业务压力较高外，大部分业务压力较小，合理设置单Index的分片数效果也不错。我们的经验是单个分片的大小在10GB~30GB之间比较合适，对于压力非常小的业务可以直接分配1个分片。其他用户可结合具体场景考虑，同时注意单分片的记录条数不要超过上限2,147,483,519。
在平衡我们的业务场景对数据可靠性的要求 及 不同副本数对存储成本的开销 两个因素之后，我们选择使用一主一从的副本策略。
目前我们集群单Index的平均分配数为3，集群的总分片数下降到3w~个。
(4)分片分配流程优化
默认情况下，ES在分配分片时会考虑分片relocation对磁盘空间的影响。在分片数较少时，这个优化处理的副作用不明显。但随着单机分片数量的上升，这个优化处理涉及的多层循环嵌套过程耗时愈发明显。可通过cluster.routing.allocation.disk.include_relocations: false关闭此功能，这对磁盘均衡程度影响不明显。
(5)预创建Index
对于单集群3w分片的场景，集中在每周某天0点创建Index，对集群的压力还是较大，且存储空间存在波动。考虑到集群的持续扩展能力和可靠性，我们采用预创建方式提前创建分片，并把按Index的创建时间均匀打散到每周的每一天。
(6)持续调整分片数
对于集群分片的调整，通常不是一蹴而就的。随着业务的发展，不断新增的子业务 或 原有子业务规模发生突变，都需要持续调整分片数量。
默认情况下，新增的子业务会有默认的分片数量，如果不足，会在测试阶段及上线初期及时发现。随着业务发展，系统会考虑Index近期的数据量、写入速度、集群规模等因素，动态调整分片数量。


## ElasticSearch如何解决深度分页的问题？

答：使用scroll（有状态）和search after（无状态）的游标方式。

## lucence倒排索引

三个文件：字典文件，频率文件，位置文件。词典文件不仅保存有每个关键词，还保留了指向频率文件和位置文件的指针，通过指针可以找到该关键字的频率信息和位置信息。

field的概念，用于表达信息所在位置（如标题中，文章中，url中），在建索引中，该field信息也记录在词典文件中，每个关键词都有一个field信息(因为每个关键字一定属于一个或多个field)。

关键字是按字符顺序排列的（lucene没有使用B树结构），因此lucene可以用二元搜索算法快速定位关键词。

假设要查询单词 “live”，lucene先对词典二元查找、找到该词，通过指向频率文件的指针读出所有文章号，然后返回结果。词典通常非常小，因而，整个过程的时间是毫秒级的。 　　

对词典文件中的关键词进行了压缩，关键词压缩为<前缀长度，后缀>，例如：当前词为“阿拉伯语”，上一个词为“阿拉伯”，那么“阿拉伯语”压缩为<3，语>。对数字的压缩，数字只保存与上一个值的差值。


## Hbase二级索引，索引海量数据实现方案？

(1) 方案1:使用开源的hbase-indexer，是借助于hbase的WAL实现，不会影响hbase性能

   https://blog.csdn.net/xiahoujie_90/article/details/53400044

(2)  方案2：基于ES自己实现，利用habse的协处理器实现，会影响hbase性能

关键注意点：因为数据是存在Hbase中，ES充当的是索引角色，所以在创建ES的mapping时，

应指定_source为enabled:false。关闭存储原始文档。


## 后台系统怎么防止请求重复提交。
前端js，控制按钮。前端放置令牌。
数据库唯一索引。redis看key是否存在。或者数据库字段状态。

## 如何做限流策略，令牌桶和漏斗算法的使用场景。
    令牌桶可以应对突发的大流量
   漏斗算法用于请求恒定速率通过


## 有没有遇到进线上GC，出现的症状是什么样的，怎么解决的？
利用堆快照，查看到底是哪些对象占用大量内存导致经常gc


## 假如你的项目出现性能瓶颈了，你觉得可能会是哪些方面，怎么解决问题。
DB层面，有可能是sql，索引，表过大。
缓存层面：有可能缓存命中率差
Java层面:代码写法
架构层面：服务器不够

## 情景题：如果一个外卖配送单子要发布，现在有200个骑手都想要接这一单，如何保证只有一个骑手接到单子？
分布式锁，或者幂等接口，CAS乐观锁

## 场景题：美团首页每天会从10000个商家里面推荐50个商家置顶，每个商家有一个权值，你如何来推荐？第二天怎么更新推荐的商家？
可以借鉴下stackoverflow，视频网站等等的推荐算法。


## 场景题：微信抢红包问题
悲观锁，乐观锁，存储过程放在mysql数据库中。


## 场景题：1000个任务，分给10个人做，你怎么分配，先在纸上写个最简单的版本，然后优化。
全局队列，把1000任务放在一个队列里面，然后每个人都是取，完成任务。
分为10个队列，每个人分别到自己对应的队列中去取务。

